import os
import glob
import json
import sys
import yaml
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List

try:
    import lakefs_sdk
    from lakefs_sdk.configuration import Configuration
    from lakefs_sdk.api_client import ApiClient
    from lakefs_sdk.api import objects_api, branches_api, commits_api
except ImportError:
    print('‚ùå LakeFS SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ –∫–æ–º–∞–Ω–¥–æ–π: pip install lakefs-sdk')
    sys.exit(1)

try:
    import mlflow
    import mlflow.pytorch
    MLFLOW_AVAILABLE = True
except ImportError:
    print('‚ùå MLflow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ –∫–æ–º–∞–Ω–¥–æ–π: pip install mlflow')
    MLFLOW_AVAILABLE = False


def load_config(config_path: str = 'config.yaml') -> Dict[str, Any]:
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except FileNotFoundError:
        print(f'‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω')
        sys.exit(1)
    except yaml.YAMLError as e:
        print(f'‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ YAML —Ñ–∞–π–ª–∞: {e}')
        sys.exit(1)


def setup_logging(config: Dict[str, Any]):
    log_config = config.get('logging', {})

    # –°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ª–æ–≥–æ–≤
    log_file = log_config.get('file', 'logs/benchmark_execution.log')
    os.makedirs(os.path.dirname(log_file), exist_ok=True)

    # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    level = getattr(logging, log_config.get('level', 'INFO').upper())

    # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä
    formatter = logging.Formatter(
        log_config.get('format', '[%(asctime)s] %(levelname)s - %(message)s')
    )

    # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ª–æ–≥–≥–µ—Ä
    logger = logging.getLogger()
    logger.setLevel(level)

    # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π —Ö—ç–Ω–¥–ª–µ—Ä
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    # –§–∞–π–ª–æ–≤—ã–π —Ö—ç–Ω–¥–ª–µ—Ä —Å —Ä–æ—Ç–∞—Ü–∏–µ–π
    from logging.handlers import RotatingFileHandler
    file_handler = RotatingFileHandler(
        log_file,
        maxBytes=log_config.get('max_file_size', 100) * 1024 * 1024, # MB –≤ –±–∞–π—Ç—ã
        backupCount=log_config.get('backup_count', 5)
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    return logger


class LakeFSUploader:
    def __init__(self, config: Dict[str, Any]):
        lakefs_config = config['lakefs']
        
        self.endpoint = lakefs_config['endpoint']
        self.repository = lakefs_config['repository']
        self.branch_prefix = lakefs_config.get('branch_prefix', 'experiment')

        access_key = lakefs_config.get('access_key')
        secret_key = lakefs_config.get('secret_key')

        if not access_key or not secret_key:
            raise ValueError("Access key –∏ secret key –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ LakeFS")

        configuration = Configuration(
            host=self.endpoint,
            username=access_key,
            password=secret_key
        )
        
        api_client = ApiClient(configuration)
        self.objects_api = objects_api.ObjectsApi(api_client)
        self.branches_api = branches_api.BranchesApi(api_client)
        self.commits_api = commits_api.CommitsApi(api_client)
        
    def upload_file(self, repo, branch, local_path, remote_path):
        try:
            import os
            file_size = os.path.getsize(local_path)
            print(f"Uploading {local_path} ({file_size} bytes) -> {remote_path}")

            with open(local_path, 'rb') as f:
                file_content = f.read()

            #import io 
            #file_stream = io.BytesIO(file_content)
            #print(type(file_content))

            self.objects_api.upload_object(
                repository=repo,
                branch=branch,
                path=remote_path,
                content=file_content
            )
            
            print(f'‚úÖ Uploaded {local_path} -> {remote_path}')
            return True
        except Exception as e:
            print(f'‚ùå Failed to upload {local_path}: {e}')
            #print(f'‚ùå Error type: {type(e).__name__}')
            return False
            
    def create_branch(self, repo, branch_name, source_branch='main'):
        try:
            self.branches_api.create_branch(
                repository=repo,
                branch_creation=lakefs_sdk.BranchCreation(
                    name=branch_name,
                    source=source_branch
                )
            )
            print(f'‚úÖ Created branch: {branch_name}')
            return True
        except Exception as e:
            print(f'‚ö†Ô∏è Branch creation failed: {e}')
            return False
            
    def commit_changes(self, repo, branch, message):
        try:
            commit = lakefs_sdk.CommitCreation(message=message)
            result = self.commits_api.commit(
                repository=repo,
                branch=branch,
                commit_creation=commit
            )
            print(f'‚úÖ Committed changes: {result.id}')
            return result.id
        except Exception as e:
            print(f'‚ùå Failed to commit: {e}')
            return None

    def check_connection(self):
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤–µ—Ç–æ–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
            from lakefs_sdk.api import repositories_api
            repos_api = repositories_api.RepositoriesApi(self.objects_api.api_client)
            repos_api.list_repositories()
            return True
        except Exception as e:
            logging.error(f'–ù–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ LakeFS: {e}')
            return False


class MLflowLogger:
    def __init__(self, config: Dict[str, Any]):
        if not MLFLOW_AVAILABLE:
            logging.warning('MLflow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è')
            self.enabled = False
            return
            
        self.enabled = True
        mlflow_config = config.get('mlflow', {})
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ MLflow
        tracking_uri = mlflow_config.get('tracking_uri')
        if tracking_uri:
            mlflow.set_tracking_uri(tracking_uri)
            
        username = mlflow_config.get('username')
        password = mlflow_config.get('password')

        os.environ['MLFLOW_TRACKING_USERNAME'] = username
        os.environ['MLFLOW_TRACKING_PASSWORD'] = password

        self.experiment_name = mlflow_config.get('experiment_name', 'spec-bench-evaluation')
        self.default_tags = config.get('experiment', {}).get('default_tags', {})
        
        # –°–æ–∑–¥–∞—Ç—å –∏–ª–∏ –ø–æ–ª—É—á–∏—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
        try:
            self.experiment = mlflow.set_experiment(self.experiment_name)
            logging.info(f'MLflow —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {self.experiment_name}')
        except Exception as e:
            logging.error(f'–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ MLflow —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {e}')
            self.enabled = False
    
    def start_run(self, run_name: str = None, tags: Dict[str, str] = None) -> Optional[str]:
        if not self.enabled:
            return None
            
        try:
            # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Ç–µ–≥–∏
            all_tags = self.default_tags.copy()
            if tags:
                all_tags.update(tags)
                
            run = mlflow.start_run(run_name=run_name, tags=all_tags)
            logging.info(f'–ù–∞—á–∞—Ç MLflow run: {run.info.run_id}')
            return run.info.run_id
        except Exception as e:
            logging.error(f'–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è MLflow run: {e}')
            return None
    
    def log_params(self, params: Dict[str, Any]):
        if not self.enabled:
            return
            
        try:
            for key, value in params.items():
                mlflow.log_param(key, value)
        except Exception as e:
            logging.error(f'–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ MLflow: {e}')
    
    def log_metrics(self, metrics: Dict[str, float], step: int = None):
        if not self.enabled:
            return
            
        try:
            for key, value in metrics.items():
                mlflow.log_metric(key, value, step=step)
        except Exception as e:
            logging.error(f'–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –≤ MLflow: {e}')
    
    def log_artifacts(self, artifact_path: str, local_path: str = None):
        if not self.enabled:
            return
            
        try:
            if local_path:
                mlflow.log_artifact(local_path, artifact_path)
            else:
                mlflow.log_artifacts(artifact_path)
        except Exception as e:
            logging.error(f'–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤ MLflow: {e}')
    
    def log_benchmark_results(self, results_file: str, benchmark_name: str):
        if not self.enabled or not os.path.exists(results_file):
            return
            
        try:
            # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞
            metrics = self._parse_benchmark_results(results_file, benchmark_name)
            
            if metrics:
                self.log_metrics(metrics)
                logging.info(f'–ó–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {benchmark_name}: {len(metrics)} –∑–Ω–∞—á–µ–Ω–∏–π')
        except Exception as e:
            logging.error(f'–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ {benchmark_name}: {e}')


    def _parse_benchmark_results(self, results_file: str, benchmark_name: str) -> Dict[str, float]:
        metrics = {}
        
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                results = [json.loads(line) for line in f if line.strip()]
            
            if not results:
                return metrics
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
            total_questions = len(results)
            metrics[f'{benchmark_name}_total_questions'] = total_questions
            
            # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            wall_times = []
            decoding_steps = []
            new_tokens = []
            accept_lengths = []
            
            for result in results:
                choices = result.get('choices', [{}])
                if choices:
                    choice = choices[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π choice
                    
                    # –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                    if 'wall_time' in choice:
                        wall_times.extend(choice['wall_time'])
                    
                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
                    if 'decoding_steps' in choice:
                        decoding_steps.extend(choice['decoding_steps'])
                    
                    # –ù–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
                    if 'new_tokens' in choice:
                        new_tokens.extend(choice['new_tokens'])
                    
                    # –ü—Ä–∏–Ω—è—Ç—ã–µ –¥–ª–∏–Ω—ã (–¥–ª—è speculative decoding)
                    if 'accept_lengths' in choice:
                        accept_lengths.extend(choice['accept_lengths'])
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
            if wall_times:
                metrics[f'{benchmark_name}_avg_wall_time'] = sum(wall_times) / len(wall_times)
                metrics[f'{benchmark_name}_total_wall_time'] = sum(wall_times)
                metrics[f'{benchmark_name}_max_wall_time'] = max(wall_times)
                metrics[f'{benchmark_name}_min_wall_time'] = min(wall_times)
            
            if decoding_steps:
                metrics[f'{benchmark_name}_avg_decoding_steps'] = sum(decoding_steps) / len(decoding_steps)
                metrics[f'{benchmark_name}_total_decoding_steps'] = sum(decoding_steps)
            
            if new_tokens:
                metrics[f'{benchmark_name}_avg_new_tokens'] = sum(new_tokens) / len(new_tokens)
                metrics[f'{benchmark_name}_total_new_tokens'] = sum(new_tokens)
                
                # –¢–æ–∫–µ–Ω—ã –≤ —Å–µ–∫—É–Ω–¥—É
                if wall_times and len(wall_times) == len(new_tokens):
                    tokens_per_sec = [nt / wt if wt > 0 else 0 for nt, wt in zip(new_tokens, wall_times)]
                    metrics[f'{benchmark_name}_avg_tokens_per_sec'] = sum(tokens_per_sec) / len(tokens_per_sec)
                    metrics[f'{benchmark_name}_max_tokens_per_sec'] = max(tokens_per_sec)
            
            if accept_lengths:
                metrics[f'{benchmark_name}_avg_accept_length'] = sum(accept_lengths) / len(accept_lengths)
                metrics[f'{benchmark_name}_max_accept_length'] = max(accept_lengths)
                
                # Acceptance rate –¥–ª—è speculative decoding
                if len(accept_lengths) > 0:
                    acceptance_rate = sum(1 for al in accept_lengths if al > 0) / len(accept_lengths)
                    metrics[f'{benchmark_name}_acceptance_rate'] = acceptance_rate
        
        except Exception as e:
            logging.error(f'–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ {results_file}: {e}')
        
        return metrics
    
    def end_run(self):
        if self.enabled:
            try:
                mlflow.end_run()
                logging.info('MLflow run –∑–∞–≤–µ—Ä—à–µ–Ω')
            except Exception as e:
                logging.error(f'–û—à–∏–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è MLflow run: {e}')


def sanitize_branch_name(name: str) -> str:
    import re

    safe_name = re.sub(r'[^a-zA-Z0-9_-]', '_', name)
    if safe_name.startswith('-'):
        safe_name = 'model_' + safe_name[1:]
    return safe_name


def get_system_info():
    '''–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–µ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö'''
    import platform
    import torch
    
    info = {
        'timestamp': datetime.now().isoformat(),
        'platform': platform.platform(),
        'python_version': platform.python_version(),
        'torch_version': torch.__version__,
        'cuda_available': torch.cuda.is_available(),
    }
    
    if torch.cuda.is_available():
        info['cuda_version'] = torch.version.cuda
        info['gpu_count'] = torch.cuda.device_count()
        info['gpu_names'] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]
    
    return info


def create_metadata_file(results_dir, uploaded_files, config):
    '''–°–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞'''
    metadata = {
        'experiment_info': get_system_info(),
        'results_directory': str(results_dir),
        'uploaded_files': uploaded_files,
        'file_count': len(uploaded_files),
        'config_snapshot': {
            'lakefs_endpoint': config['lakefs']['endpoint'],
            'lakefs_repository': config['lakefs']['repository'],
            'mlflow_tracking_uri': config.get('mlflow', {}).get('tracking_uri'),
            'experiment_tags': config.get('experiment', {}).get('default_tags', {})
        }
    }
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    total_size = 0
    for file_path in uploaded_files:
        local_path = os.path.join(results_dir, os.path.basename(file_path.split('/')[-1]))
        if os.path.exists(local_path):
            total_size += os.path.getsize(local_path)
    
    metadata['total_size_bytes'] = total_size
    metadata['total_size_mb'] = round(total_size / (1024 * 1024), 2)
    
    return metadata


def upload_single_result(config: Dict[str, Any], result_file: str, model_name: str = None):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤ LakeFS"""
    
    if not os.path.exists(result_file):
        logging.error(f"–§–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {result_file}")
        return False
    
    logging.info(f"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ LakeFS: {os.path.basename(result_file)}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LakeFS –∫–ª–∏–µ–Ω—Ç–∞
    try:
        uploader = LakeFSUploader(config)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LakeFS –∫–ª–∏–µ–Ω—Ç–∞: {e}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
    if not uploader.check_connection():
        return False
    
    # –°–æ–∑–¥–∞—Ç—å –≤–µ—Ç–∫—É –¥–ª—è —ç—Ç–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    timestamp = datetime.now().strftime("%Y%m%d")
    if model_name:
        safe_model_name = sanitize_branch_name(model_name)
        experiment_branch = f"{uploader.branch_prefix}_{safe_model_name}_{timestamp}"
    else:
        experiment_branch = f"{uploader.branch_prefix}_single_{timestamp}"
    
    # –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞—Ç—å –≤–µ—Ç–∫—É
    uploader.create_branch(uploader.repository, experiment_branch, "main")
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª
    filename = os.path.basename(result_file)
    remote_path = f"results/{timestamp}/{filename}"
    
    if uploader.upload_file(uploader.repository, experiment_branch, result_file, remote_path):
        # –°–æ–∑–¥–∞—Ç—å –∫–æ–º–º–∏—Ç
        commit_message = f"Single result upload: {filename}\nTimestamp: {timestamp}"
        if model_name:
            commit_message += f"\nModel: {model_name}"
        
        commit_id = uploader.commit_changes(uploader.repository, experiment_branch, commit_message)
        
        if commit_id:
            logging.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            logging.info(f"üìä –í–µ—Ç–∫–∞: {experiment_branch}")
            logging.info(f"üìù –ö–æ–º–º–∏—Ç: {commit_id}")
            logging.info(f"üåê –ü—Ä–æ—Å–º–æ—Ç—Ä: {uploader.endpoint}/repositories/{uploader.repository}/objects?ref={experiment_branch}")
            return True
        else:
            logging.error("‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–º–º–∏—Ç–∞")
            return False
    else:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {result_file}")
        return False


def upload_benchmark_results(config: Dict[str, Any], results_dir: str = None, mlflow_logger: MLflowLogger = None):
    '''–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –≤ LakeFS –∏ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –≤ MLflow'''
    
    if results_dir is None:
        results_dir = config.get('benchmark', {}).get('results_dir', 'data/spec_bench/model_answer')
    
    logging.info('üöÄ –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ LakeFS...')
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LakeFS –∫–ª–∏–µ–Ω—Ç–∞
    try:
        uploader = LakeFSUploader(config)
    except Exception as e:
        logging.error(f'–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LakeFS –∫–ª–∏–µ–Ω—Ç–∞: {e}')
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
    if not uploader.check_connection():
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if not os.path.exists(results_dir):
        logging.error(f'–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {results_dir}')
        return False
    
    # –ù–∞–π—Ç–∏ –≤—Å–µ —Ñ–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    pattern = os.path.join(results_dir, '*.jsonl')
    result_files = glob.glob(pattern)
    
    if not result_files:
        logging.warning(f'–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {results_dir}')
        logging.info(f'–ò—â—É —Ñ–∞–π–ª—ã –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É: {pattern}')
        return False
    
    logging.info(f'üìÅ –ù–∞–π–¥–µ–Ω–æ {len(result_files)} —Ñ–∞–π–ª–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    for f in result_files:
        logging.info(f'  - {os.path.basename(f)}')
    
    # –°–æ–∑–¥–∞—Ç—å –≤–µ—Ç–∫—É –¥–ª—è —ç—Ç–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    experiment_branch = f'{uploader.branch_prefix}_{timestamp}'
    
    # –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞—Ç—å –≤–µ—Ç–∫—É
    uploader.create_branch(uploader.repository, experiment_branch, 'main')
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow
    if mlflow_logger:
        run_tags = {
            'experiment_timestamp': timestamp,
            'lakefs_branch': experiment_branch,
            'lakefs_repository': uploader.repository,
            'results_count': str(len(result_files))
        }
        mlflow_logger.start_run(run_name=f'benchmark_{timestamp}', tags=run_tags)
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        system_info = get_system_info()
        mlflow_logger.log_params(system_info)
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª—ã
    uploaded_files = []
    failed_files = []
    
    for local_file in result_files:
        filename = os.path.basename(local_file)
        remote_path = f'results/{timestamp}/{filename}'
        
        if uploader.upload_file(uploader.repository, experiment_branch, local_file, remote_path):
            uploaded_files.append(remote_path)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞ –≤ MLflow
            if mlflow_logger:
                benchmark_name = filename.replace('.jsonl', '').replace('-', '_')
                mlflow_logger.log_benchmark_results(local_file, benchmark_name)
        else:
            failed_files.append(local_file)
    
    # –°–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    if uploaded_files:
        metadata = create_metadata_file(results_dir, uploaded_files, config)
        metadata_path = f'results/{timestamp}/experiment_metadata.json'
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω–æ –∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å
        temp_metadata_file = os.path.join(results_dir, 'temp_metadata.json')
        with open(temp_metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        if uploader.upload_file(uploader.repository, experiment_branch, temp_metadata_file, metadata_path):
            uploaded_files.append(metadata_path)
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –≤ MLflow
        if mlflow_logger:
            mlflow_logger.log_artifacts('metadata', temp_metadata_file)
        
        # –£–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        os.remove(temp_metadata_file)
    
    # –°–æ–∑–¥–∞—Ç—å –∫–æ–º–º–∏—Ç
    if uploaded_files:
        commit_message = f'Benchmark results from {timestamp}\n\n'
        commit_message += f'Uploaded {len(uploaded_files)} files:\n'
        commit_message += '\n'.join(f'- {f}' for f in uploaded_files)
        
        if failed_files:
            commit_message += f'\n\nFailed to upload {len(failed_files)} files:\n'
            commit_message += '\n'.join(f'- {os.path.basename(f)}' for f in failed_files)
        
        commit_id = uploader.commit_changes(uploader.repository, experiment_branch, commit_message)
        
        if commit_id:
            logging.info(f'üéâ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(uploaded_files)} —Ñ–∞–π–ª–æ–≤!')
            logging.info(f'üìä –í–µ—Ç–∫–∞: {experiment_branch}')
            logging.info(f'üìù –ö–æ–º–º–∏—Ç: {commit_id}')
            logging.info(f'üåê –ü—Ä–æ—Å–º–æ—Ç—Ä: {uploader.endpoint}/repositories/{uploader.repository}/objects?ref={experiment_branch}')
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∏—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ MLflow
            if mlflow_logger:
                final_metrics = {
                    'uploaded_files_count': len(uploaded_files),
                    'failed_files_count': len(failed_files),
                    'total_files_count': len(result_files),
                    'upload_success_rate': len(uploaded_files) / len(result_files) if result_files else 0
                }
                mlflow_logger.log_metrics(final_metrics)
                mlflow_logger.log_params({
                    'lakefs_commit_id': commit_id,
                    'lakefs_branch': experiment_branch,
                    'lakefs_repository': uploader.repository
                })
            
            if failed_files:
                logging.warning(f'–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {len(failed_files)} —Ñ–∞–π–ª–æ–≤:')
                for f in failed_files:
                    logging.warning(f'  - {os.path.basename(f)}')
            
            return True
    
    logging.error('–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª—ã –≤ LakeFS')
    return False


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –≤ LakeFS –∏ MLflow')
    parser.add_argument(
        '--config', 
        default='config.yaml',
        help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: config.yaml)'
    )
    parser.add_argument(
        '--results-dir', 
        help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é)'
    )
    parser.add_argument(
        '--no-mlflow',
        action='store_true',
        help='–û—Ç–∫–ª—é—á–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow'
    )
    parser.add_argument(
        "--single-file",
        help="–ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–¥–∏–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–π–ª –≤–º–µ—Å—Ç–æ –≤—Å–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"
    )
    parser.add_argument(
        "--model-name",
        help="–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞)"
    )
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = load_config(args.config)
    
    # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    logger = setup_logging(config)
    
    try:
        if args.single_file:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            success = upload_single_result(
                config=config,
                result_file=args.single_file,
                model_name=args.model_name
            )
        else:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MLflow –ª–æ–≥–≥–µ—Ä–∞
            mlflow_logger = None
            if not args.no_mlflow:
                mlflow_logger = MLflowLogger(config)
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            success = upload_benchmark_results(
                config=config,
                results_dir=args.results_dir,
                mlflow_logger=mlflow_logger
            )
            
            # –ó–∞–≤–µ—Ä—à–∏—Ç—å MLflow run
            if mlflow_logger:
                mlflow_logger.end_run()
        
        if success:
            logging.info('üéâ –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!')
        else:
            logging.error('‚ùå –û–ø–µ—Ä–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã —Å –æ—à–∏–±–∫–∞–º–∏')
        
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        logging.info('–û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º')
        if mlflow_logger:
            mlflow_logger.end_run()
        sys.exit(130)
    except Exception as e:
        logging.error(f'–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}')
        if mlflow_logger:
            mlflow_logger.end_run()
        sys.exit(1)


if __name__ == '__main__':
    main()