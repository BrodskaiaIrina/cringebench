#!/bin/bash

check_config() {
    local config_file="${1:-config.yaml}"
    
    if [[ ! -f "$config_file" ]]; then
        log "‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª $config_file –Ω–µ –Ω–∞–π–¥–µ–Ω"
        log "–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª –Ω–∞ –æ—Å–Ω–æ–≤–µ config.yaml.example"
        return 1
    fi
    
    return 0
}

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
Vicuna_PATH=./models/vicuna-7b-v1.3
Eagle_PATH=./models/EAGLE-Vicuna-7B-v1.3
Eagle3_PATH=./models/EAGLE3-Vicuna1.3-13B
Medusa_PATH=./models/medusa-vicuna-7b-v1.3
Hydra_PATH=./models/hydra-vicuna-7b-v1.3
Drafter_PATH=./models/vicuna-68m
Space_PATH=./models/vicuna-v1.3-7b-space
datastore_PATH=./model/rest/datastore/datastore_chat_large.idx

Vicuna_PATH_tknz="lmsys/vicuna-7b-v1.3"

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
MODEL_NAME=vicuna-7b-v1.3
TEMP=0.0
GPU_DEVICES=0
bench_NAME="spec_bench"
torch_dtype="float16" # ["float32", "float64", "float16", "bfloat16"]

# –°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
RESULTS_DIR="data/${bench_NAME}/model_answer"
mkdir -p "${RESULTS_DIR}"

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
LOG_FILE="evaluation_$(date +%Y%m%d_%H%M%S).log"
echo "üìù –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª: ${LOG_FILE}"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "${LOG_FILE}"
}


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–¥–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞ —Å –ø–æ—à–∞–≥–æ–≤—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
run_benchmark_with_logging() {
    local cmd="$1"
    local name="$2"
    local model_id="$3"
    local config_file="$4"
    local start_time
    local end_time
    local duration
    
    log "üöÄ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞: ${name}"
    start_time=$(date +%s)
    
    # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–º–∞–Ω–¥—É
    eval $cmd
    local exit_code=$?
    
    end_time=$(date +%s)
    duration=$((end_time - start_time))
    
    if [ $exit_code -eq 0 ]; then
        log "‚úÖ ${name} –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞ ${duration} —Å–µ–∫—É–Ω–¥"
        
        # –ù–∞–π—Ç–∏ —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
        local result_file="${RESULTS_DIR}/${model_id}.jsonl"
        if [[ -f "$result_file" ]]; then
            log "üìÅ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: $(basename "$result_file")"
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ LakeFS
            upload_single_result "$config_file" "$result_file" "$model_id"
            
            # –ù–∞–π—Ç–∏ baseline —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏
            local baseline_file="${RESULTS_DIR}/${MODEL_NAME}-vanilla-${torch_dtype}-temp-${TEMP}.jsonl"
            if [[ -f "$baseline_file" ]]; then
                log "üìä –ù–∞–π–¥–µ–Ω baseline –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: $(basename "$baseline_file")"
                
                # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—É—Ç—å –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É (–∏—Å–ø–æ–ª—å–∑—É–µ–º Vicuna_PATH)
                local tokenizer_path="$Vicuna_PATH"
                
                # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow
                analyze_speed "$config_file" "$model_id" "$result_file" "$baseline_file" "$tokenizer_path"
            else
                log "‚ö†Ô∏è Baseline —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏: $(basename "$baseline_file")"
                log "   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ baseline (vanilla) –º–æ–¥–µ–ª—å –±—ã–ª–∞ –∑–∞–ø—É—â–µ–Ω–∞ –ø–µ—Ä–≤–æ–π"
            fi
        else
            log "‚ùå –§–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: $(basename "$result_file")"
        fi
        
        return 0
    else
        log "‚ùå ${name} –∑–∞–≤–µ—Ä—à–µ–Ω —Å –æ—à–∏–±–∫–æ–π (–∫–æ–¥: ${exit_code}) –∑–∞ ${duration} —Å–µ–∫—É–Ω–¥"
        return $exit_code
    fi
}


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–¥–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞ (—Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
run_benchmark() {
    local cmd="$1"
    local name="$2"
    local start_time
    local end_time
    local duration
    
    log "üöÄ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞: ${name}"
    start_time=$(date +%s)
    
    # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–º–∞–Ω–¥—É
    eval $cmd
    local exit_code=$?
    
    end_time=$(date +%s)
    duration=$((end_time - start_time))
    
    if [ $exit_code -eq 0 ]; then
        log "‚úÖ ${name} –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞ ${duration} —Å–µ–∫—É–Ω–¥"
        return 0
    else
        log "‚ùå ${name} –∑–∞–≤–µ—Ä—à–µ–Ω —Å –æ—à–∏–±–∫–æ–π (–∫–æ–¥: ${exit_code}) –∑–∞ ${duration} —Å–µ–∫—É–Ω–¥"
        return $exit_code
    fi
}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª–∏ (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–ª–∏ –∑–∞–ø—É—Å–∫ inference)
process_model() {
    local model_name="$1"
    local model_id="$2"
    local result_file="$3"
    local inference_cmd="$4"
    local config_file="$5"
    local run_speed_analysis="${6:-true}"  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é true
    
    if [[ -f "$result_file" ]]; then
        log "üìÅ –ù–∞–π–¥–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: $model_name"
        log "‚ö° –ü—Ä–æ–ø—É—Å–∫ inference, –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞"
        upload_single_result "$config_file" "$result_file" "$model_id"
        
        # –ê–Ω–∞–ª–∏–∑ —Å–∫–æ—Ä–æ—Å—Ç–∏ –¥–ª—è –Ω–µ-baseline –º–æ–¥–µ–ª–µ–π
        if [[ "$run_speed_analysis" == "true" ]]; then
            local baseline_file="${RESULTS_DIR}/${MODEL_NAME}-vanilla-${torch_dtype}-temp-${TEMP}.jsonl"
            if [[ -f "$baseline_file" ]]; then
                analyze_speed "$config_file" "$model_id" "$result_file" "$baseline_file" "$Vicuna_PATH_tknz"
            else
                log "‚ö†Ô∏è Baseline —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏"
            fi
        fi
    else
        log "üöÄ –ó–∞–ø—É—Å–∫: $model_name"
        eval "$inference_cmd"
        if [ $? -eq 0 ]; then
            log "‚úÖ $model_name –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ"
            upload_single_result "$config_file" "$result_file" "$model_id"
            
            # –ê–Ω–∞–ª–∏–∑ —Å–∫–æ—Ä–æ—Å—Ç–∏ –¥–ª—è –Ω–µ-baseline –º–æ–¥–µ–ª–µ–π
            if [[ "$run_speed_analysis" == "true" ]]; then
                local baseline_file="${RESULTS_DIR}/${MODEL_NAME}-vanilla-${torch_dtype}-temp-${TEMP}.jsonl"
                if [[ -f "$baseline_file" ]]; then
                    analyze_speed "$config_file" "$model_id" "$result_file" "$baseline_file" "$Vicuna_PATH_tknz"
                else
                    log "‚ö†Ô∏è Baseline —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏"
                fi
            fi
        else
            log "‚ùå $model_name –∑–∞–≤–µ—Ä—à–µ–Ω —Å –æ—à–∏–±–∫–æ–π"
        fi
    fi
}


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –æ–¥–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ LakeFS
upload_single_result() {
    local config_file="$1"
    local result_file="$2"
    local model_name="$3"
    
    if check_config "$config_file"; then
        log "üì§ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ LakeFS: $(basename "$result_file")"
        python3 upload_results.py --config "$config_file" --single-file "$result_file" --model-name "$model_name"
        local upload_exit_code=$?
        
        if [ $upload_exit_code -eq 0 ]; then
            log "‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –≤ LakeFS"
        else
            log "‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ (–∫–æ–¥: ${upload_exit_code})"
        fi
        return $upload_exit_code
    else
        log "‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏ (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)"
        return 1
    fi
}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
analyze_speed() {
    local config_file="$1"
    local model_name="$2"
    local model_file="$3"
    local baseline_file="$4"
    local tokenizer_path="$5"
    
    if check_config "$config_file"; then
        log "üìä –ê–Ω–∞–ª–∏–∑ —Å–∫–æ—Ä–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏: $model_name"
        python3 speed_mlflow.py --config "$config_file" --model-name "$model_name" \
            --model-file "$model_file" --baseline-file "$baseline_file" \
            --tokenizer-path "$tokenizer_path"
        local speed_exit_code=$?
        
        if [ $speed_exit_code -eq 0 ]; then
            log "‚úÖ –ê–Ω–∞–ª–∏–∑ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ"
        else
            log "‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ (–∫–æ–¥: ${speed_exit_code})"
        fi
        return $speed_exit_code
    else
        log "‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)"
        return 1
    fi
}


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ LakeFS –∏ MLflow (—Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è)
upload_to_services() {
    local config_file="${1:-config.yaml}"
    
    if check_config "$config_file"; then
        log "üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ LakeFS –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow..."
        python3 upload_results.py --config "$config_file" --results-dir "${RESULTS_DIR}"
        local upload_exit_code=$?
        
        if [ $upload_exit_code -eq 0 ]; then
            log "‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ LakeFS –∏ MLflow"
        else
            log "‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ (–∫–æ–¥: ${upload_exit_code})"
        fi
        return $upload_exit_code
    else
        log "‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏ (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)"
        return 1
    fi
}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
check_model_path() {
    local path="$1"
    local name="$2"
    
    if [[ "$path" == "/your_own_path/"* ]]; then
        log "‚ö†Ô∏è ${name}: –ü—É—Ç—å –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω (${path})"
        return 1
    elif [[ ! -d "$path" ]]; then
        log "‚ùå ${name}: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (${path})"
        return 1
    else
        log "‚úÖ ${name}: –ú–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ (${path})"
        return 0
    fi
}

# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
main() {
    local config_file="${1:-config.yaml}"
    
    log "üéØ –ù–∞—á–∞–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è Spec-Bench Evaluation"
    log "üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:"
    log "   - –ú–æ–¥–µ–ª—å: ${MODEL_NAME}"
    log "   - –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: ${TEMP}"
    log "   - –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: ${torch_dtype}"
    log "   - GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: ${GPU_DEVICES}"
    log "   - –ë–µ–Ω—á–º–∞—Ä–∫: ${bench_NAME}"
    log "   - –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: ${RESULTS_DIR}"
    log "   - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª: ${config_file}"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if check_config "$config_file"; then
        log "‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ"
    else
        log "‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –≤–Ω–µ—à–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã"
    fi
    
    # –°—á–µ—Ç—á–∏–∫–∏
    local total_benchmarks=0
    local successful_benchmarks=0
    local failed_benchmarks=0
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ —Å –ø–æ—à–∞–≥–æ–≤—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    # Baseline (Vanilla)
    if check_model_path "$Vicuna_PATH" "Baseline"; then
        process_model "Baseline (Vanilla)" \
                     "${MODEL_NAME}-vanilla-${torch_dtype}-temp-${TEMP}" \
                     "${RESULTS_DIR}/${MODEL_NAME}-vanilla-${torch_dtype}-temp-${TEMP}.jsonl" \
                     "CUDA_VISIBLE_DEVICES=${GPU_DEVICES} python -m evaluation.inference_baseline -model-path $Vicuna_PATH -model-id ${MODEL_NAME}-vanilla-${torch_dtype}-temp-${TEMP} -bench-name $bench_NAME -temperature $TEMP -dtype $torch_dtype" \
                     "$config_file" \
                     "false"
    fi
    
    # SPS (Speculative Sampling)
    # if check_model_path "$Vicuna_PATH" "SPS" && check_model_path "$Drafter_PATH" "Drafter"; then
    #     process_model "SPS (Speculative Sampling)" \
    #                  "${MODEL_NAME}-sps-68m-${torch_dtype}-temp-${TEMP}" \
    #                  "${RESULTS_DIR}/${MODEL_NAME}-sps-68m-${torch_dtype}-temp-${TEMP}.jsonl" \
    #                  "CUDA_VISIBLE_DEVICES=${GPU_DEVICES} python -m evaluation.inference_sps -model-path $Vicuna_PATH -drafter-path $Drafter_PATH -model-id ${MODEL_NAME}-sps-68m-${torch_dtype}-temp-${TEMP} -bench-name $bench_NAME -temperature $TEMP -dtype $torch_dtype" \
    #                  "$config_file"
    # fi
    
    # Medusa
    # if check_model_path "$Medusa_PATH" "Medusa"; then
    #     process_model "Medusa" \
    #                  "${MODEL_NAME}-medusa-${torch_dtype}" \
    #                  "${RESULTS_DIR}/${MODEL_NAME}-medusa-${torch_dtype}.jsonl" \
    #                  "CUDA_VISIBLE_DEVICES=${GPU_DEVICES} python -m evaluation.inference_medusa -model-path $Medusa_PATH -base-model $Vicuna_PATH -model-id ${MODEL_NAME}-medusa-${torch_dtype} -bench-name $bench_NAME -temperature $TEMP -dtype $torch_dtype" \
    #                  "$config_file"
    # fi
    
    # # EAGLE
    # if check_model_path "$Eagle_PATH" "EAGLE"; then
    #     process_model "EAGLE" \
    #                  "${MODEL_NAME}-eagle-${torch_dtype}" \
    #                  "${RESULTS_DIR}/${MODEL_NAME}-eagle-${torch_dtype}.jsonl" \
    #                  "CUDA_VISIBLE_DEVICES=${GPU_DEVICES} python -m evaluation.inference_eagle -ea-model-path $Eagle_PATH -base-model-path $Vicuna_PATH -model-id ${MODEL_NAME}-eagle-${torch_dtype} -bench-name $bench_NAME -temperature $TEMP -dtype $torch_dtype" \
    #                  "$config_file"
    # fi
    
    # EAGLE2
    # if check_model_path "$Eagle_PATH" "EAGLE2"; then
    #     process_model "EAGLE2" \
    #                  "${MODEL_NAME}-eagle2-${torch_dtype}" \
    #                  "${RESULTS_DIR}/${MODEL_NAME}-eagle2-${torch_dtype}.jsonl" \
    #                  "CUDA_VISIBLE_DEVICES=${GPU_DEVICES} python -m evaluation.inference_eagle2 -ea-model-path $Eagle_PATH -base-model-path $Vicuna_PATH -model-id ${MODEL_NAME}-eagle2-${torch_dtype} -bench-name $bench_NAME -temperature $TEMP -dtype $torch_dtype" \
    #                  "$config_file"
    # fi
    
    # # EAGLE3
    # if check_model_path "$Eagle3_PATH" "EAGLE3"; then
    #     process_model "EAGLE3" \
    #                  "${MODEL_NAME}-eagle3-${torch_dtype}" \
    #                  "${RESULTS_DIR}/${MODEL_NAME}-eagle3-${torch_dtype}.jsonl" \
    #                  "CUDA_VISIBLE_DEVICES=${GPU_DEVICES} python -m evaluation.inference_eagle3 -ea-model-path $Eagle3_PATH -base-model-path $Vicuna_PATH -model-id ${MODEL_NAME}-eagle3-${torch_dtype} -bench-name $bench_NAME -temperature $TEMP -dtype $torch_dtype" \
    #                  "$config_file"
    # fi
    
    # LADE (Lookahead)
    # if check_model_path "$Vicuna_PATH" "LADE"; then
    #     process_model "LADE (Lookahead)" \
    #                  "${MODEL_NAME}-lade-level-5-win-7-guess-7-${torch_dtype}" \
    #                  "${RESULTS_DIR}/${MODEL_NAME}-lade-level-5-win-7-guess-7-${torch_dtype}.jsonl" \
    #                  "CUDA_VISIBLE_DEVICES=${GPU_DEVICES} USE_LADE=1 python -m evaluation.inference_lookahead -model-path $Vicuna_PATH -model-id ${MODEL_NAME}-lade-level-5-win-7-guess-7-${torch_dtype} -level 5 -window 7 -guess 7 -bench-name $bench_NAME -dtype $torch_dtype" \
    #                  "$config_file"
    # fi
    
    # # PLD (Parallel Decoding)
    # if check_model_path "$Vicuna_PATH" "PLD"; then
    #     process_model "PLD (Parallel Decoding)" \
    #                  "${MODEL_NAME}-pld-${torch_dtype}" \
    #                  "${RESULTS_DIR}/${MODEL_NAME}-pld-${torch_dtype}.jsonl" \
    #                  "CUDA_VISIBLE_DEVICES=${GPU_DEVICES} python -m evaluation.inference_pld -model-path $Vicuna_PATH -model-id ${MODEL_NAME}-pld-${torch_dtype} -bench-name $bench_NAME -dtype $torch_dtype" \
    #                  "$config_file"
    # fi
    
    # REST (Retrieval-based Speculative Decoding)
    # if check_model_path "$Vicuna_PATH" "REST" && [[ -f "$datastore_PATH" ]]; then
    #     process_model "REST (Retrieval-based Speculative Decoding)" \
    #                  "${MODEL_NAME}-rest-${torch_dtype}" \
    #                  "${RESULTS_DIR}/${MODEL_NAME}-rest-${torch_dtype}.jsonl" \
    #                  "CUDA_VISIBLE_DEVICES=${GPU_DEVICES} RAYON_NUM_THREADS=6 python -m evaluation.inference_rest -model-path $Vicuna_PATH -model-id ${MODEL_NAME}-rest-${torch_dtype} -datastore-path $datastore_PATH -bench-name $bench_NAME -temperature $TEMP -dtype $torch_dtype" \
    #                  "$config_file"
    # fi
    
    # # Hydra
    # if check_model_path "$Hydra_PATH" "Hydra"; then
    #     process_model "Hydra" \
    #                  "${MODEL_NAME}-hydra-${torch_dtype}" \
    #                  "${RESULTS_DIR}/${MODEL_NAME}-hydra-${torch_dtype}.jsonl" \
    #                  "CUDA_VISIBLE_DEVICES=${GPU_DEVICES} python -m evaluation.inference_hydra -model-path $Hydra_PATH -base-model $Vicuna_PATH -model-id ${MODEL_NAME}-hydra-${torch_dtype} -bench-name $bench_NAME -temperature $TEMP -dtype $torch_dtype" \
    #                  "$config_file"
    # fi
    
    # # SPACE
    # if check_model_path "$Space_PATH" "SPACE"; then
    #     process_model "SPACE" \
    #                  "${MODEL_NAME}-space-${torch_dtype}-temp-${TEMP}" \
    #                  "${RESULTS_DIR}/${MODEL_NAME}-space-${torch_dtype}-temp-${TEMP}.jsonl" \
    #                  "CUDA_VISIBLE_DEVICES=${GPU_DEVICES} python -m evaluation.inference_space -model-path $Space_PATH -model-id ${MODEL_NAME}-space-${torch_dtype}-temp-${TEMP} -bench-name $bench_NAME -temperature $TEMP -dtype $torch_dtype" \
    #                  "$config_file"
    # fi
    
    # Recycling
    # if check_model_path "$Vicuna_PATH" "Recycling"; then
    #     process_model "Recycling" \
    #                  "${MODEL_NAME}-recycling" \
    #                  "${RESULTS_DIR}/${MODEL_NAME}-recycling.jsonl" \
    #                  "CUDA_VISIBLE_DEVICES=${GPU_DEVICES} python -m evaluation.inference_recycling -model-path $Vicuna_PATH -model-id ${MODEL_NAME}-recycling -bench-name $bench_NAME -temperature $TEMP -dtype $torch_dtype" \
    #                  "$config_file"
    # fi
    
    # SAMD
    # if check_model_path "$Vicuna_PATH" "SAMD" && check_model_path "$Eagle_PATH" "SAMD tree model"; then
    #     process_model "SAMD" \
    #                  "${MODEL_NAME}-samd" \
    #                  "${RESULTS_DIR}/${MODEL_NAME}-samd.jsonl" \
    #                  "CUDA_VISIBLE_DEVICES=${GPU_DEVICES} python -m evaluation.inference_samd -model-path $Vicuna_PATH -model-id ${MODEL_NAME}-samd -bench-name $bench_NAME -temperature $TEMP -dtype $torch_dtype -samd_n_predicts 40 -samd_len_threshold 5 -samd_len_bias 5 -tree_method eagle2 -attn_implementation sdpa -tree_model_path $Eagle_PATH" \
    #                  "$config_file"
    # fi

    # log "–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!"
    # log "–ü–æ–ª–Ω—ã–π –ª–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ ${LOG_FILE}"
}

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤
cleanup() {
    log "üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã..."
    exit 130
}

trap cleanup SIGINT SIGTERM

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
check_dependencies() {
    local missing_deps=()
    
    if ! command -v python3 &> /dev/null; then
        missing_deps+=("python")
    fi
    
    if ! python3 -c "import torch" &> /dev/null; then
        missing_deps+=("torch")
    fi
    
    if [[ ${#missing_deps[@]} -gt 0 ]]; then
        log "‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: ${missing_deps[*]}"
        log "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∏—Ö –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º"
        exit 1
    fi
}

# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    echo "üöÄ Spec-Bench Evaluation Script —Å LakeFS –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π"
    echo "=================================================="
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    check_dependencies
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
    main "$@"
fi 